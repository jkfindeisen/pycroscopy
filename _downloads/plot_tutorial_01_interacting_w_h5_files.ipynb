{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n================================================\nTutorial 1: Interacting with Pycroscopy H5 files\n================================================\nSuhas Somnath\n\n11/11/2017\n\nThis set of tutorials will serve as examples for using and developing end-to-end workflows for pycroscopy.\n\nIn this example, we will learn how to interact with pycroscopy formatted h5 files.\n\nIntroduction\n============\nWe highly recommend reading about the pycroscopy data format - available in the docs.\n\nPycroscopy uses a data-centric approach to data analysis and processing meaning that results from all data analysis and\nprocessing are written to the same h5 file that contains the recorded measurements. The Hierarchical Data Format (HDF5)\nallows data to be stored in multiple datasets in a tree-like manner. However, certain rules and considerations have\nbeen made in pycroscopy to ensure consistent and easy access to any data. pycroscopy.hdf_utils contains a lot of\nutility functions that simplify access to data and this tutorial provides an overview of many of the these functions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n# Warning package in case something goes wrong\nfrom warnings import warn\n# Package for downloading online files:\ntry:\n    # This package is not part of anaconda and may need to be installed.\n    import wget\nexcept ImportError:\n    warn('wget not found.  Will install with pip.')\n    import pip\n    pip.main(['install', 'wget'])\n    import wget\nimport h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\ntry:\n    import pycroscopy as px\nexcept ImportError:\n    warn('pycroscopy not found.  Will install with pip.')\n    import pip\n    pip.main(['install', 'pycroscopy'])\n    import pycroscopy as px"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Downloading the example file from the pycroscopy Github project\nurl = 'https://raw.githubusercontent.com/pycroscopy/pycroscopy/master/data/BEPS_small.h5'\nh5_path = 'temp.h5'\n_ = wget.download(url, h5_path)\n\nprint('Working on:\\n' + h5_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pycroscopy uses the h5py python package to access the HDF5 files and its contents.\nConventionally, the h5py package is used to create, read, write, and modify h5 files.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Open the file in read-only mode\nh5_f = h5py.File(h5_path, mode='r')\n\n# We can also use the ioHDF5 class from Pycroscopy to open the file.  Note that you do need to close the\n# file in h5py before opening it again.\nh5_f.close()\nhdf = px.ioHDF5(h5_path)\nh5_f = hdf.file\n\n# Here, h5_f is an active handle to the open file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect the contents of this h5 data file\n=========================================\n\nThe file contents are stored in a tree structure, just like files on a contemporary computer. The file contains\ndatagroups (similar to file folders) and datasets (similar to spreadsheets).\nThere are several datasets in the file and these store:\n\n* The actual measurement collected from the experiment\n* Spatial location on the sample where each measurement was collected\n* Information to support and explain the spectral data collected at each location\n* Since pycroscopy stores results from processing and analyses performed on the data in the same file, these\n  datasets and datagroups are present as well\n* Any other relevant ancillary information\n\nSoon after opening any file, it is often of interest to list the contents of the file. While one can use the open\nsource software HDFViewer developed by the HDF organization, pycroscopy.hdf_utils also has a simply utility to\nquickly visualize all the datasets and datagroups within the file within python.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Contents of the H5 file:')\npx.hdf_utils.print_tree(h5_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accessing datasets and datagroups\n==================================\n\nThere are numerous ways to access datasets and datagroups in H5 files. First we show the methods using native h5py\nfunctionality.\n\nDatasets and datagroups can be accessed by specifying the path, just like a web page or a file in a directory\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Selecting a datagroup by specifying the absolute path:\nh5_meas_group = h5_f['Measurement_000']\nprint('h5_meas_group:', h5_meas_group)\nprint('h5_chan_group:', h5_f['Measurement_000/Channel_000'])\n\n# Selecting a dataset by specifying the absolute path:\nh5_fft = h5_f['Measurement_000/Channel_000/Bin_FFT']\nprint('h5_fft:', h5_fft)\n\n# Selecting the same dataset using the relative path.\n# First we get \"Channel_000\" from h5_meas_group:\nh5_group = h5_meas_group['Channel_000']\n\n# Now we access Bin_FFT from within h5_group:\nh5_fft = h5_group['Bin_FFT']\nprint('h5_fft:', h5_fft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The datagroup \"Channel_000\" contains several \"members\", where these members could be datasets like \"Bin_FFT\" or\ndatagroups like \"Channel_000\"\n\nThe output above shows that the \"Bin_FFT\" dataset is a one dimensional dataset, and has complex value (a +bi)\nentries at each element in the 1D array.\nThis dataset is contained in a datagroup called \"Channel_000\" which itself is contained in a datagroup called\n\"Measurement_000\"\n\nAnd here's two methods using pycroscopy.hdf_utils\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Specific match of dataset name:\nudvs_dsets_1 = px.hdf_utils.getDataSet(h5_f, 'UDVS')\nfor item in udvs_dsets_1:\n    print(item)\n\n# This function returns all datasets that match even a portion of the name\nudvs_dsets_2 = px.hdf_utils.findDataset(h5_f, 'UDVS')\nfor item in udvs_dsets_2:\n    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pycroscopy hdf5 files contain three kinds of datasets:\n\n* Main datasets that contain data recorded / computed at multiple spatial locations.\n* Ancillary datasets that support a main dataset\n* Other datasets\n\nFor more information, please refer to the documentation on the pycroscopy data format.\n\nWe can check which datasets within h5_group are Main datasets using a handy hdf_utils function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for dset_name in h5_group:\n    print(px.hdf_utils.checkIfMain(h5_group[dset_name]), ':\\t', dset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data of interest is almost always contained within Main Datasets. Thus, while all three kinds of datasets can\nbe accessed using the methods shown above, we have a function in hdf_utils that allows us to only list the main\ndatasets within the file / group:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main_dsets = px.hdf_utils.get_all_main(h5_f)\nfor dset in main_dsets:\n    print(dset.name, dset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The datasets above show that the file contains three main datasets. Two of these datasets are contained in a folder\ncalled Raw_Data-SHO_Fit_000 meaning that they are results of an operation called SHO_Fit performed on the main\ndataset Raw_Data. The first of the three main datasets is indeed the Raw_Data dataset from which the latter\ntwo datasets (Fit and Guess) were derived.\n\nPycroscopy allows the same operation, such as 'SHO_Fit', to be performed on the same dataset (Raw_Data), multiple\ntimes. Each time the operation is performed, a new datagroup is created to hold the new results. Often, we may\nwant to perform a few operations such as:\n\n* Find the (source / main) dataset from which certain results were derived\n* Check if a particular operation was performed on a main dataset\n* Find all datagroups corresponding to a particular operation (e.g. - SHO_Fit) being applied to a main dataset\n\nhdf_utils has a few handy functions that simply many of these use cases:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First get the dataset corresponding to Raw_Data\nh5_raw = h5_f['/Measurement_000/Channel_000/Raw_Data']\n\nprint('Instances of operation \"{}\" applied to dataset named \"{}\":'.format('SHO_Fit', h5_raw.name))\nh5_sho_group_list = px.hdf_utils.findH5group(h5_raw, 'SHO_Fit')\nprint(h5_sho_group_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the SHO_Fit operation was performed on Raw_Data only once, which is why findH5group returned only one\ndatagroup - SHO_Fit_000.\n\nOften one may want to check if a certain operation was performed on a dataset with the very same parameters to\navoid recomputing the results. hdf_utils has a function for this too:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Parameters already used for computing SHO_Fit on Raw_Data in the file:')\nprint(px.hdf_utils.get_attributes(h5_f['/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000']))\nprint('\\nChecking to see if SHO Fits have been computed on the raw dataset:')\nprint('Using pycroscopy')\nprint(px.hdf_utils.check_for_old(h5_raw, 'SHO_Fit',\n                                 new_parms={'SHO_fit_method': 'pycroscopy BESHO'}))\nprint('Using BEAM')\nprint(px.hdf_utils.check_for_old(h5_raw, 'SHO_Fit',\n                                 new_parms={'SHO_fit_method': 'BEAM BESHO'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clearly, while findH5group returned any and all groups corresponding to SHO_Fit being applied to Raw_Data,\ncheck_for_old only returned the group(s) where the operation was performed using the same parameters.\n\nLet's consider the inverse scenario where we are interested in finding the source dataset from which the known\nresult was derived:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h5_sho_group = h5_sho_group_list[0]\nprint('Datagroup containing the SHO fits:')\nprint(h5_sho_group)\nprint('\\nDataset on which the SHO Fit was computed:')\nh5_source_dset = px.hdf_utils.get_source_dataset(h5_sho_group)\nprint(h5_source_dset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accessing Attributes:\n=====================\n\nHDF5 datasets and datagroups can also store metadata such as experimental parameters. These metadata can be text,\nnumbers, small lists of numbers or text etc. These metadata can be very important for understanding the datasets\nand guide the analysis routines.\n\nh5py offers a basic method for accessing attributes attached to datasets and datagroups. However, more complicated\noperations such as accessing multiple attributes or accessing the original string value of string attributes can\nbe problematic in python 3. pycroscopy.hdf_utils has a few functions that simplifies the process of accessing\nattributes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Listing all attributes using get_attributes:\nattr_dict = px.hdf_utils.get_attributes(h5_meas_group, attr_names=None)\nfor att_name in attr_dict:\n    print('{} : {}'.format(att_name, attr_dict[att_name]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# accessing specific attributes only:\nprint(px.hdf_utils.get_attributes(h5_meas_group, attr_names=['VS_mode', 'BE_phase_content']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing the number value of attributes is not a problem using h5py:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# via the standard h5py library:\nprint(h5_meas_group.attrs['VS_amplitude_[V]'])\nprint(h5_meas_group.attrs['VS_amplitude_[V]'] == 8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, accessing string valued attributes and using them for comparison is a problem using the standard h5py\nlibrary\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(h5_meas_group.attrs['VS_measure_in_field_loops'])\n\n# comparing the (byte)string value of attributes is a problem with python 3:\nh5_meas_group.attrs['VS_measure_in_field_loops'] == 'in and out-of-field'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "the get_attr function in hdf_utils handles such string complications by itself:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "str_val = px.hdf_utils.get_attr(h5_meas_group, 'VS_measure_in_field_loops')\nprint(str_val == 'in and out-of-field')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main Datasets via PycroDataset\n==============================\n\nFor this example, we will be working with a Band Excitation Polarization Switching (BEPS) dataset acquired from\nadvanced atomic force microscopes. In the much simpler Band Excitation (BE) imaging datasets, a single spectra is\nacquired at each location in a two dimensional grid of spatial locations. Thus, BE imaging datasets have two\nposition dimensions (X, Y) and one spectroscopic dimension (frequency - against which the spectra is recorded).\nThe BEPS dataset used in this example has a spectra for each combination of three other parameters (DC offset,\nField, and Cycle). Thus, this dataset has three new spectral dimensions in addition to the spectra itself. Hence,\nthis dataset becomes a 2+4 = 6 dimensional dataset\n\nIn pycroscopy, all spatial dimensions are collapsed to a single dimension and similarly, all spectroscopic\ndimensions are also collapsed to a single dimension. Thus, the data is stored as a two-dimensional (N x P)\nmatrix with N spatial locations each with P spectroscopic datapoints.\n\nThis general and intuitive format allows imaging data from any instrument, measurement scheme, size, or\ndimensionality to be represented in the same way. Such an instrument independent data format enables a single\nset of analysis and processing functions to be reused for multiple image formats or modalities.\n\nMain datasets can be thought of as substantially more capable and information-packed than standard datasets\nsince they have (or are linked to) all the necessary information to describe a measured dataset. The additional\ninformation contained / linked by Main datasets includes:\n\n* the recorded physical quantity\n* units of the data\n* names of the position and spectroscopic dimensions\n* dimensionality of the data in its original N dimensional form etc.\n\nWhile it is most certainly possible to access this information via the native h5py functionality, it can become\ntedious very quickly.  Pycroscopy's PycroDataset class makes such necessary information and any necessary\nfunctionality easily accessible.\n\nPycroDataset objects are still h5py.Dataset objects underneath, like all datasets accessed above, but add an\nadditional layer of functionality to simplify data operations. Let's compare the information we can get via the\nstandard h5py library with that from PycroDataset to see the additional layer of functionality. The PycroDataset\nmakes the spectral and positional dimensions, sizes immediately apparent among other things.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Accessing the raw data\npycro_main = main_dsets[0]\nprint('Dataset as observed via h5py:')\nprint()\nprint('\\nDataset as seen via a PycroDataset object:')\nprint(pycro_main)\n# Showing that the PycroDataset is still just a h5py.Dataset object underneath:\nprint()\nprint(isinstance(pycro_main, h5py.Dataset))\nprint(pycro_main == h5_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main datasets are often linked to supporting datasets in addition to the mandatory ancillary datasets.  The main\ndataset contains attributes which are references to these datasets\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for att_name in pycro_main.attrs:\n    print(att_name, pycro_main.attrs[att_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These datasets can be accessed easily via a handy hdf_utils function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(px.hdf_utils.getAuxData(pycro_main, auxDataName='Bin_FFT'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The additional functionality of PycroDataset is enabled through several functions in hdf_utils. Below, we provide\nseveral such examples along with comparisons with performing the same operations in a simpler manner using\nthe PycroDataset object:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# A function to describe the nature of the contents within a dataset\nprint(px.hdf_utils.get_data_descriptor(h5_raw))\n\n# this functionality can be accessed in PycroDatasets via:\nprint(pycro_main.data_descriptor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using Ancillary Datasets\n========================\n\nAs mentioned earlier, the ancillary datasets contain information about the dimensionality of the original\nN-dimensional dataset.  Here we see how we can extract the size and corresponding names of each of the spectral\nand position dimensions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We can use the getAuxData function again to get the ancillary datasets linked with the main dataset:\n# The [0] slicing is to take the one and only position indices and spectroscopic indices linked with the dataset\nh5_pos_inds = px.hdf_utils.getAuxData(h5_raw, auxDataName='Position_Indices')[0]\nh5_spec_inds = px.hdf_utils.getAuxData(h5_raw, auxDataName='Spectroscopic_Indices')[0]\n\n# Need to state that the array needs to be of the spectral shape.\nprint('Spectroscopic dimensions:')\nprint(px.hdf_utils.get_formatted_labels(h5_spec_inds))\nprint('Size of each dimension:')\nprint(px.hdf_utils.get_dimensionality(h5_spec_inds))\nprint('Position dimensions:')\nprint(px.hdf_utils.get_formatted_labels(h5_pos_inds))\nprint('Size of each dimension:')\nprint(px.hdf_utils.get_dimensionality(h5_pos_inds[()].T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The same tasks can very easily be accomplished via the PycroDataset object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# an alternate way to get the spectroscopic indices is simply via:\nprint(pycro_main.h5_spec_inds)\n\n# We can get the spectral / position labels and dimensions easily via:\nprint('Spectroscopic dimensions:')\nprint(pycro_main.spec_dim_descriptors)\nprint('Size of each dimension:')\nprint(pycro_main.spec_dim_sizes)\nprint('Position dimensions:')\nprint(pycro_main.pos_dim_descriptors)\nprint('Size of each dimension:')\nprint(pycro_main.pos_dim_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a few cases, the spectroscopic / position dimensions are not arranged in descending order of rate of change.\nIn other words, the dimensions in these ancillary matrices are not arranged from fastest-varying to slowest.\nTo account for such discrepancies, hdf_utils has a very handy function that goes through each of the columns or\nrows in the ancillary indices matrices and finds the order in which these dimensions vary.\n\nBelow we illustrate an example of sorting the names of the spectroscopic dimensions from fastest to slowest in\na BEPS data file:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spec_sort_order = px.hdf_utils.get_sort_order(h5_spec_inds)\nprint('Spectroscopic dimensions arranged as is:')\nunsorted_spec_labels = px.hdf_utils.get_formatted_labels(h5_spec_inds)\nprint(unsorted_spec_labels)\nsorted_spec_labels = np.array(unsorted_spec_labels)[np.array(spec_sort_order)]\nprint('Spectroscopic dimensions arranged from fastest to slowest')\nprint(sorted_spec_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When visualizing the data it is essential to plot the data against appropriate values on the X, Y, Z axes.\nExtracting a simple list or array of values to plot against may be challenging especially for multidimensional\ndataset such as the one under consideration. Fortunately, hdf_utils has a very handy function for this as well:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h5_spec_inds = px.hdf_utils.getAuxData(pycro_main, auxDataName='Spectroscopic_Indices')[0]\nh5_spec_vals = px.hdf_utils.getAuxData(pycro_main, auxDataName='Spectroscopic_Values')[0]\ndimension_name = 'DC_Offset'\ndc_dict = px.hdf_utils.get_unit_values(h5_spec_inds, h5_spec_vals, dim_names=dimension_name)\nprint(dc_dict)\ndc_val = dc_dict[dimension_name]\n\nfig, axis = plt.subplots()\naxis.plot(dc_val)\naxis.set_title(dimension_name)\naxis.set_xlabel('Points in dimension')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Yet again, this process is simpler when using the PycroDataset object:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dv_val = pycro_main.get_spec_values(dim_name=dimension_name)\n\nfig, axis = plt.subplots()\naxis.plot(dc_val)\naxis.set_title(dimension_name)\naxis.set_xlabel('Points in dimension')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reshaping Data\n==============\n\nPycroscopy stores N dimensional datasets in a flattened 2D form of position x spectral values. It can become\nchallenging to retrieve the data in its original N-dimensional form, especially for multidimensional datasets\nsuch as the one we are working on. Fortunately, all the information regarding the dimensionality of the dataset\nare contained in the spectral and position ancillary datasets. hdf_utils has a very useful function that can\nhelp retrieve the N-dimensional form of the data using a simple function call:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ndim_form, success, labels = px.hdf_utils.reshape_to_Ndims(h5_raw, get_labels=True)\nif success:\n    print('Succeeded in reshaping flattened 2D dataset to N dimensions')\n    print('Shape of the data in its original 2D form')\n    print(h5_raw.shape)\n    print('Shape of the N dimensional form of the dataset:')\n    print(ndim_form.shape)\n    print('And these are the dimensions')\n    print(labels)\nelse:\n    print('Failed in reshaping the dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The whole process is simplified further when using the PycroDataset object:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ndim_form = pycro_main.get_n_dim_form()\nprint('Shape of the N dimensional form of the dataset:')\nprint(ndim_form.shape)\nprint('And these are the dimensions')\nprint(pycro_main.n_dim_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "two_dim_form, success = px.hdf_utils.reshape_from_Ndims(ndim_form,\n                                                        h5_pos=h5_pos_inds,\n                                                        h5_spec=h5_spec_inds)\nif success:\n    print('Shape of flattened two dimensional form')\n    print(two_dim_form.shape)\nelse:\n    print('Failed in flattening the N dimensional dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Close and delete the h5_file\nh5_f.close()\nos.remove(h5_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}