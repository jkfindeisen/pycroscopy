

.. _sphx_glr_auto_publications_plot_tutorial_01_interacting_w_h5_files.py:


================================================
Tutorial 1: Interacting with Pycroscopy H5 files
================================================
Suhas Somnath

11/11/2017

This set of tutorials will serve as examples for using and developing end-to-end workflows for pycroscopy.

In this example, we will learn how to interact with pycroscopy formatted h5 files.

Introduction
============
We highly recommend reading about the pycroscopy data format - available in the docs.

Pycroscopy uses a data-centric approach to data analysis and processing meaning that results from all data analysis and
processing are written to the same h5 file that contains the recorded measurements. The Hierarchical Data Format (HDF5)
allows data to be stored in multiple datasets in a tree-like manner. However, certain rules and considerations have
been made in pycroscopy to ensure consistent and easy access to any data. pycroscopy.hdf_utils contains a lot of
utility functions that simplify access to data and this tutorial provides an overview of many of the these functions



.. code-block:: python


    import os
    # Warning package in case something goes wrong
    from warnings import warn
    # Package for downloading online files:
    try:
        # This package is not part of anaconda and may need to be installed.
        import wget
    except ImportError:
        warn('wget not found.  Will install with pip.')
        import pip
        pip.main(['install', 'wget'])
        import wget
    import h5py
    import numpy as np
    import matplotlib.pyplot as plt
    try:
        import pycroscopy as px
    except ImportError:
        warn('pycroscopy not found.  Will install with pip.')
        import pip
        pip.main(['install', 'pycroscopy'])
        import pycroscopy as px








.. code-block:: python


    # Downloading the example file from the pycroscopy Github project
    url = 'https://raw.githubusercontent.com/pycroscopy/pycroscopy/master/data/BEPS_small.h5'
    h5_path = 'temp.h5'
    _ = wget.download(url, h5_path)

    print('Working on:\n' + h5_path)





.. rst-class:: sphx-glr-script-out

 Out::

    Working on:
    temp.h5


Pycroscopy uses the h5py python package to access the HDF5 files and its contents.
Conventionally, the h5py package is used to create, read, write, and modify h5 files.



.. code-block:: python


    # Open the file in read-only mode
    h5_f = h5py.File(h5_path, mode='r')

    # We can also use the ioHDF5 class from Pycroscopy to open the file.  Note that you do need to close the
    # file in h5py before opening it again.
    h5_f.close()
    hdf = px.ioHDF5(h5_path)
    h5_f = hdf.file

    # Here, h5_f is an active handle to the open file







Inspect the contents of this h5 data file
=========================================

The file contents are stored in a tree structure, just like files on a contemporary computer. The file contains
datagroups (similar to file folders) and datasets (similar to spreadsheets).
There are several datasets in the file and these store:

* The actual measurement collected from the experiment
* Spatial location on the sample where each measurement was collected
* Information to support and explain the spectral data collected at each location
* Since pycroscopy stores results from processing and analyses performed on the data in the same file, these
  datasets and datagroups are present as well
* Any other relevant ancillary information

Soon after opening any file, it is often of interest to list the contents of the file. While one can use the open
source software HDFViewer developed by the HDF organization, pycroscopy.hdf_utils also has a simply utility to
quickly visualize all the datasets and datagroups within the file within python.



.. code-block:: python


    print('Contents of the H5 file:')
    px.hdf_utils.print_tree(h5_f)





.. rst-class:: sphx-glr-script-out

 Out::

    Contents of the H5 file:
    /
    Measurement_000
    Measurement_000/Channel_000
    Measurement_000/Channel_000/Bin_FFT
    Measurement_000/Channel_000/Bin_Frequencies
    Measurement_000/Channel_000/Bin_Indices
    Measurement_000/Channel_000/Bin_Step
    Measurement_000/Channel_000/Bin_Wfm_Type
    Measurement_000/Channel_000/Excitation_Waveform
    Measurement_000/Channel_000/Noise_Floor
    Measurement_000/Channel_000/Position_Indices
    Measurement_000/Channel_000/Position_Values
    Measurement_000/Channel_000/Raw_Data
    Measurement_000/Channel_000/Raw_Data-SHO_Fit_000
    Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Fit
    Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Guess
    Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Indices
    Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Spectroscopic_Values
    Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000
    Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Bin_Frequencies
    Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Mean_Spectrogram
    Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Spectroscopic_Parameter
    Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_000/Step_Averaged_Response
    Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001
    Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Bin_Frequencies
    Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Mean_Spectrogram
    Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Spectroscopic_Parameter
    Measurement_000/Channel_000/Spatially_Averaged_Plot_Group_001/Step_Averaged_Response
    Measurement_000/Channel_000/Spectroscopic_Indices
    Measurement_000/Channel_000/Spectroscopic_Values
    Measurement_000/Channel_000/UDVS
    Measurement_000/Channel_000/UDVS_Indices


Accessing datasets and datagroups
==================================

There are numerous ways to access datasets and datagroups in H5 files. First we show the methods using native h5py
functionality.

Datasets and datagroups can be accessed by specifying the path, just like a web page or a file in a directory



.. code-block:: python


    # Selecting a datagroup by specifying the absolute path:
    h5_meas_group = h5_f['Measurement_000']
    print('h5_meas_group:', h5_meas_group)
    print('h5_chan_group:', h5_f['Measurement_000/Channel_000'])

    # Selecting a dataset by specifying the absolute path:
    h5_fft = h5_f['Measurement_000/Channel_000/Bin_FFT']
    print('h5_fft:', h5_fft)

    # Selecting the same dataset using the relative path.
    # First we get "Channel_000" from h5_meas_group:
    h5_group = h5_meas_group['Channel_000']

    # Now we access Bin_FFT from within h5_group:
    h5_fft = h5_group['Bin_FFT']
    print('h5_fft:', h5_fft)





.. rst-class:: sphx-glr-script-out

 Out::

    h5_meas_group: <HDF5 group "/Measurement_000" (1 members)>
    h5_chan_group: <HDF5 group "/Measurement_000/Channel_000" (17 members)>
    h5_fft: <HDF5 dataset "Bin_FFT": shape (87,), type "<c8">
    h5_fft: <HDF5 dataset "Bin_FFT": shape (87,), type "<c8">


The datagroup "Channel_000" contains several "members", where these members could be datasets like "Bin_FFT" or
datagroups like "Channel_000"

The output above shows that the "Bin_FFT" dataset is a one dimensional dataset, and has complex value (a +bi)
entries at each element in the 1D array.
This dataset is contained in a datagroup called "Channel_000" which itself is contained in a datagroup called
"Measurement_000"

And here's two methods using pycroscopy.hdf_utils



.. code-block:: python


    # Specific match of dataset name:
    udvs_dsets_1 = px.hdf_utils.getDataSet(h5_f, 'UDVS')
    for item in udvs_dsets_1:
        print(item)

    # This function returns all datasets that match even a portion of the name
    udvs_dsets_2 = px.hdf_utils.findDataset(h5_f, 'UDVS')
    for item in udvs_dsets_2:
        print(item)





.. rst-class:: sphx-glr-script-out

 Out::

    <HDF5 dataset "UDVS": shape (256, 7), type "<f4">
    ['Measurement_000/Channel_000/UDVS', <HDF5 dataset "UDVS": shape (256, 7), type "<f4">]
    ['Measurement_000/Channel_000/UDVS_Indices', <HDF5 dataset "UDVS_Indices": shape (22272,), type "<u8">]


Pycroscopy hdf5 files contain three kinds of datasets:

* Main datasets that contain data recorded / computed at multiple spatial locations.
* Ancillary datasets that support a main dataset
* Other datasets

For more information, please refer to the documentation on the pycroscopy data format.

We can check which datasets within h5_group are Main datasets using a handy hdf_utils function:



.. code-block:: python


    for dset_name in h5_group:
        print(px.hdf_utils.checkIfMain(h5_group[dset_name]), ':\t', dset_name)





.. rst-class:: sphx-glr-script-out

 Out::

    False :  Bin_FFT
    False :  Bin_Frequencies
    False :  Bin_Indices
    False :  Bin_Step
    False :  Bin_Wfm_Type
    False :  Excitation_Waveform
    False :  Noise_Floor
    False :  Position_Indices
    False :  Position_Values
    True :   Raw_Data
    False :  Raw_Data-SHO_Fit_000
    False :  Spatially_Averaged_Plot_Group_000
    False :  Spatially_Averaged_Plot_Group_001
    False :  Spectroscopic_Indices
    False :  Spectroscopic_Values
    False :  UDVS
    False :  UDVS_Indices


The data of interest is almost always contained within Main Datasets. Thus, while all three kinds of datasets can
be accessed using the methods shown above, we have a function in hdf_utils that allows us to only list the main
datasets within the file / group:



.. code-block:: python


    main_dsets = px.hdf_utils.get_all_main(h5_f)
    for dset in main_dsets:
        print(dset.name, dset.shape)





.. rst-class:: sphx-glr-script-out

 Out::

    /Measurement_000/Channel_000/Raw_Data (25, 22272)
    /Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Fit (25, 256)
    /Measurement_000/Channel_000/Raw_Data-SHO_Fit_000/Guess (25, 256)


The datasets above show that the file contains three main datasets. Two of these datasets are contained in a folder
called Raw_Data-SHO_Fit_000 meaning that they are results of an operation called SHO_Fit performed on the main
dataset Raw_Data. The first of the three main datasets is indeed the Raw_Data dataset from which the latter
two datasets (Fit and Guess) were derived.

Pycroscopy allows the same operation, such as 'SHO_Fit', to be performed on the same dataset (Raw_Data), multiple
times. Each time the operation is performed, a new datagroup is created to hold the new results. Often, we may
want to perform a few operations such as:

* Find the (source / main) dataset from which certain results were derived
* Check if a particular operation was performed on a main dataset
* Find all datagroups corresponding to a particular operation (e.g. - SHO_Fit) being applied to a main dataset

hdf_utils has a few handy functions that simply many of these use cases:



.. code-block:: python


    # First get the dataset corresponding to Raw_Data
    h5_raw = h5_f['/Measurement_000/Channel_000/Raw_Data']

    print('Instances of operation "{}" applied to dataset named "{}":'.format('SHO_Fit', h5_raw.name))
    h5_sho_group_list = px.hdf_utils.findH5group(h5_raw, 'SHO_Fit')
    print(h5_sho_group_list)





.. rst-class:: sphx-glr-script-out

 Out::

    Instances of operation "SHO_Fit" applied to dataset named "/Measurement_000/Channel_000/Raw_Data":
    [<HDF5 group "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000" (4 members)>]


As expected, the SHO_Fit operation was performed on Raw_Data only once, which is why findH5group returned only one
datagroup - SHO_Fit_000.

Often one may want to check if a certain operation was performed on a dataset with the very same parameters to
avoid recomputing the results. hdf_utils has a function for this too:



.. code-block:: python


    print('Parameters already used for computing SHO_Fit on Raw_Data in the file:')
    print(px.hdf_utils.get_attributes(h5_f['/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000']))
    print('\nChecking to see if SHO Fits have been computed on the raw dataset:')
    print('Using pycroscopy')
    print(px.hdf_utils.check_for_old(h5_raw, 'SHO_Fit',
                                     new_parms={'SHO_fit_method': 'pycroscopy BESHO'}))
    print('Using BEAM')
    print(px.hdf_utils.check_for_old(h5_raw, 'SHO_Fit',
                                     new_parms={'SHO_fit_method': 'BEAM BESHO'}))





.. rst-class:: sphx-glr-script-out

 Out::

    Parameters already used for computing SHO_Fit on Raw_Data in the file:
    {'SHO_guess_method': 'pycroscopy BESHO', 'timestamp': '2017_08_22-15_02_08', 'machine_id': 'mac109728.ornl.gov', 'SHO_fit_method': 'pycroscopy BESHO'}

    Checking to see if SHO Fits have been computed on the raw dataset:
    Using pycroscopy
    <HDF5 group "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000" (4 members)>
    Using BEAM
    None


Clearly, while findH5group returned any and all groups corresponding to SHO_Fit being applied to Raw_Data,
check_for_old only returned the group(s) where the operation was performed using the same parameters.

Let's consider the inverse scenario where we are interested in finding the source dataset from which the known
result was derived:



.. code-block:: python


    h5_sho_group = h5_sho_group_list[0]
    print('Datagroup containing the SHO fits:')
    print(h5_sho_group)
    print('\nDataset on which the SHO Fit was computed:')
    h5_source_dset = px.hdf_utils.get_source_dataset(h5_sho_group)
    print(h5_source_dset)





.. rst-class:: sphx-glr-script-out

 Out::

    Datagroup containing the SHO fits:
    <HDF5 group "/Measurement_000/Channel_000/Raw_Data-SHO_Fit_000" (4 members)>

    Dataset on which the SHO Fit was computed:
    <HDF5 dataset "Raw_Data": shape (25, 22272), type "<c8">
    located at: 
    /Measurement_000/Channel_000/Raw_Data 
    Data contains: 
    Unknown quantity (unknown units) 
    Data dimensions and original shape: 
    Position Dimensions: 
    X - size: 5 
    Y - size: 5 
    Spectroscopic Dimensions: 
    Frequency - size: 87 
    DC_Offset - size: 64 
    Field - size: 2 
    Cycle - size: 2


Accessing Attributes:
=====================

HDF5 datasets and datagroups can also store metadata such as experimental parameters. These metadata can be text,
numbers, small lists of numbers or text etc. These metadata can be very important for understanding the datasets
and guide the analysis routines.

h5py offers a basic method for accessing attributes attached to datasets and datagroups. However, more complicated
operations such as accessing multiple attributes or accessing the original string value of string attributes can
be problematic in python 3. pycroscopy.hdf_utils has a few functions that simplifies the process of accessing
attributes



.. code-block:: python


    # Listing all attributes using get_attributes:
    attr_dict = px.hdf_utils.get_attributes(h5_meas_group, attr_names=None)
    for att_name in attr_dict:
        print('{} : {}'.format(att_name, attr_dict[att_name]))





.. rst-class:: sphx-glr-script-out

 Out::

    IO_DAQ_platform : NI 6115
    grid_measuring : 0
    BE_amplitude_[V] : 1.5
    grid_/single : grid
    BE_auto_smoothing : auto smoothing on
    VS_cycle_phase_shift : 0
    VS_steps_per_full_cycle : 64
    IO_Analog_Input_1 : +/- .1V, FFT
    File_date_and_time : 26-Feb-2015 14:49:48
    BE_phase_content : chirp-sinc hybrid
    FORC_V_high2_[V] : 10
    BE_actual_duration_[s] : 0.004
    grid_contact_set_point_[V] : 0.75
    IO_Analog_Input_4 : off
    grid_transit_set_point_[V] : 0
    num_pix : 25
    grid_num_cols : 5
    IO_Analog_Input_3 : off
    num_udvs_steps : 256
    FORC_num_of_FORC_cycles : 1
    BE_bins_per_band : 0
    FORC_num_of_FORC_repeats : 1
    BE_points_per_BE_wave : 0
    BE_phase_variation : 1
    BE_desired_duration_[s] : 0.004
    IO_Analog_Input_2 : off
    VS_mode : DC modulation mode
    grid_current_col : 1
    IO_rate_[Hz] : 2000000
    grid_num_rows : 5
    FORC_V_low2_[V] : -10
    grid_current_row : 1
    grid_transit_time_[s] : 0.1
    BE_band_edge_smoothing_[s] : 2560.5
    grid_cycle_time_[s] : 10
    IO_AO_range_[V] : +/- 10
    IO_AO_amplifier : 1
    grid_total_time_[h;m;s] : 10
    data_type : BEPSData
    BE_band_edge_trim : 0.14614
    VS_step_edge_smoothing_[s] : 0.001
    BE_repeats : 2
    File_file_name : in_out_BEPS_5x5
    VS_offset_[V] : 0
    VS_set_pulse_duration[s] : 0.002
    BE_center_frequency_[Hz] : 385000
    File_file_suffix : 3
    grid_time_remaining_[h;m;s] : 10
    VS_number_of_cycles : 2
    FORC_V_high1_[V] : 1
    grid_settle_time_[s] : 0.05
    num_bins : 22272
    grid_moving : 0
    BE_band_width_[Hz] : 60000
    File_file_path : V:\Users\Sangmo\KTaO3\150226\onSTO650\
    FORC_V_low1_[V] : -1
    VS_amplitude_[V] : 8
    File_MDAQ_version : MDAQ_VS_111114_01
    VS_cycle_fraction : full
    VS_read_voltage_[V] : 0
    VS_set_pulse_amplitude[V] : 0
    VS_measure_in_field_loops : in and out-of-field



.. code-block:: python


    # accessing specific attributes only:
    print(px.hdf_utils.get_attributes(h5_meas_group, attr_names=['VS_mode', 'BE_phase_content']))





.. rst-class:: sphx-glr-script-out

 Out::

    {'VS_mode': 'DC modulation mode', 'BE_phase_content': 'chirp-sinc hybrid'}


Comparing the number value of attributes is not a problem using h5py:



.. code-block:: python


    # via the standard h5py library:
    print(h5_meas_group.attrs['VS_amplitude_[V]'])
    print(h5_meas_group.attrs['VS_amplitude_[V]'] == 8)





.. rst-class:: sphx-glr-script-out

 Out::

    8
    True


However, accessing string valued attributes and using them for comparison is a problem using the standard h5py
library



.. code-block:: python


    print(h5_meas_group.attrs['VS_measure_in_field_loops'])

    # comparing the (byte)string value of attributes is a problem with python 3:
    h5_meas_group.attrs['VS_measure_in_field_loops'] == 'in and out-of-field'





.. rst-class:: sphx-glr-script-out

 Out::

    b'in and out-of-field'


the get_attr function in hdf_utils handles such string complications by itself:



.. code-block:: python


    str_val = px.hdf_utils.get_attr(h5_meas_group, 'VS_measure_in_field_loops')
    print(str_val == 'in and out-of-field')





.. rst-class:: sphx-glr-script-out

 Out::

    True


Main Datasets via PycroDataset
==============================

For this example, we will be working with a Band Excitation Polarization Switching (BEPS) dataset acquired from
advanced atomic force microscopes. In the much simpler Band Excitation (BE) imaging datasets, a single spectra is
acquired at each location in a two dimensional grid of spatial locations. Thus, BE imaging datasets have two
position dimensions (X, Y) and one spectroscopic dimension (frequency - against which the spectra is recorded).
The BEPS dataset used in this example has a spectra for each combination of three other parameters (DC offset,
Field, and Cycle). Thus, this dataset has three new spectral dimensions in addition to the spectra itself. Hence,
this dataset becomes a 2+4 = 6 dimensional dataset

In pycroscopy, all spatial dimensions are collapsed to a single dimension and similarly, all spectroscopic
dimensions are also collapsed to a single dimension. Thus, the data is stored as a two-dimensional (N x P)
matrix with N spatial locations each with P spectroscopic datapoints.

This general and intuitive format allows imaging data from any instrument, measurement scheme, size, or
dimensionality to be represented in the same way. Such an instrument independent data format enables a single
set of analysis and processing functions to be reused for multiple image formats or modalities.

Main datasets can be thought of as substantially more capable and information-packed than standard datasets
since they have (or are linked to) all the necessary information to describe a measured dataset. The additional
information contained / linked by Main datasets includes:

* the recorded physical quantity
* units of the data
* names of the position and spectroscopic dimensions
* dimensionality of the data in its original N dimensional form etc.

While it is most certainly possible to access this information via the native h5py functionality, it can become
tedious very quickly.  Pycroscopy's PycroDataset class makes such necessary information and any necessary
functionality easily accessible.

PycroDataset objects are still h5py.Dataset objects underneath, like all datasets accessed above, but add an
additional layer of functionality to simplify data operations. Let's compare the information we can get via the
standard h5py library with that from PycroDataset to see the additional layer of functionality. The PycroDataset
makes the spectral and positional dimensions, sizes immediately apparent among other things.



.. code-block:: python


    # Accessing the raw data
    pycro_main = main_dsets[0]
    print('Dataset as observed via h5py:')
    print()
    print('\nDataset as seen via a PycroDataset object:')
    print(pycro_main)
    # Showing that the PycroDataset is still just a h5py.Dataset object underneath:
    print()
    print(isinstance(pycro_main, h5py.Dataset))
    print(pycro_main == h5_raw)





.. rst-class:: sphx-glr-script-out

 Out::

    Dataset as observed via h5py:


    Dataset as seen via a PycroDataset object:
    <HDF5 dataset "Raw_Data": shape (25, 22272), type "<c8">
    located at: 
    /Measurement_000/Channel_000/Raw_Data 
    Data contains: 
    Unknown quantity (unknown units) 
    Data dimensions and original shape: 
    Position Dimensions: 
    X - size: 5 
    Y - size: 5 
    Spectroscopic Dimensions: 
    Frequency - size: 87 
    DC_Offset - size: 64 
    Field - size: 2 
    Cycle - size: 2

    True
    False


Main datasets are often linked to supporting datasets in addition to the mandatory ancillary datasets.  The main
dataset contains attributes which are references to these datasets



.. code-block:: python


    for att_name in pycro_main.attrs:
        print(att_name, pycro_main.attrs[att_name])





.. rst-class:: sphx-glr-script-out

 Out::

    Excitation_Waveform <HDF5 object reference>
    Position_Indices <HDF5 object reference>
    Position_Values <HDF5 object reference>
    Spectroscopic_Indices <HDF5 object reference>
    UDVS <HDF5 object reference>
    Bin_Step <HDF5 object reference>
    Bin_Indices <HDF5 object reference>
    UDVS_Indices <HDF5 object reference>
    Bin_Frequencies <HDF5 object reference>
    Bin_FFT <HDF5 object reference>
    Bin_Wfm_Type <HDF5 object reference>
    in_field_Plot_Group <HDF5 region reference>
    out_of_field_Plot_Group <HDF5 region reference>
    Noise_Floor <HDF5 object reference>
    Spectroscopic_Values <HDF5 object reference>


These datasets can be accessed easily via a handy hdf_utils function:



.. code-block:: python


    print(px.hdf_utils.getAuxData(pycro_main, auxDataName='Bin_FFT'))





.. rst-class:: sphx-glr-script-out

 Out::

    [<HDF5 dataset "Bin_FFT": shape (87,), type "<c8">]


The additional functionality of PycroDataset is enabled through several functions in hdf_utils. Below, we provide
several such examples along with comparisons with performing the same operations in a simpler manner using
the PycroDataset object:



.. code-block:: python


    # A function to describe the nature of the contents within a dataset
    print(px.hdf_utils.get_data_descriptor(h5_raw))

    # this functionality can be accessed in PycroDatasets via:
    print(pycro_main.data_descriptor)





.. rst-class:: sphx-glr-script-out

 Out::

    Unknown quantity (unknown units)
    Unknown quantity (unknown units)


Using Ancillary Datasets
========================

As mentioned earlier, the ancillary datasets contain information about the dimensionality of the original
N-dimensional dataset.  Here we see how we can extract the size and corresponding names of each of the spectral
and position dimensions.



.. code-block:: python


    # We can use the getAuxData function again to get the ancillary datasets linked with the main dataset:
    # The [0] slicing is to take the one and only position indices and spectroscopic indices linked with the dataset
    h5_pos_inds = px.hdf_utils.getAuxData(h5_raw, auxDataName='Position_Indices')[0]
    h5_spec_inds = px.hdf_utils.getAuxData(h5_raw, auxDataName='Spectroscopic_Indices')[0]

    # Need to state that the array needs to be of the spectral shape.
    print('Spectroscopic dimensions:')
    print(px.hdf_utils.get_formatted_labels(h5_spec_inds))
    print('Size of each dimension:')
    print(px.hdf_utils.get_dimensionality(h5_spec_inds))
    print('Position dimensions:')
    print(px.hdf_utils.get_formatted_labels(h5_pos_inds))
    print('Size of each dimension:')
    print(px.hdf_utils.get_dimensionality(h5_pos_inds[()].T))





.. rst-class:: sphx-glr-script-out

 Out::

    Spectroscopic dimensions:
    ['Frequency (Hz)', 'DC_Offset (V)', 'Field ()', 'Cycle ()']
    Size of each dimension:
    [87, 64, 2, 2]
    Position dimensions:
    ['X (m)', 'Y (m)']
    Size of each dimension:
    [5, 5]


The same tasks can very easily be accomplished via the PycroDataset object



.. code-block:: python


    # an alternate way to get the spectroscopic indices is simply via:
    print(pycro_main.h5_spec_inds)

    # We can get the spectral / position labels and dimensions easily via:
    print('Spectroscopic dimensions:')
    print(pycro_main.spec_dim_descriptors)
    print('Size of each dimension:')
    print(pycro_main.spec_dim_sizes)
    print('Position dimensions:')
    print(pycro_main.pos_dim_descriptors)
    print('Size of each dimension:')
    print(pycro_main.pos_dim_sizes)





.. rst-class:: sphx-glr-script-out

 Out::

    <HDF5 dataset "Spectroscopic_Indices": shape (4, 22272), type "<i4">
    Spectroscopic dimensions:
    ['Frequency (Hz)', 'DC_Offset (V)', 'Field ()', 'Cycle ()']
    Size of each dimension:
    [87, 64, 2, 2]
    Position dimensions:
    ['X (m)', 'Y (m)']
    Size of each dimension:
    [5, 5]


In a few cases, the spectroscopic / position dimensions are not arranged in descending order of rate of change.
In other words, the dimensions in these ancillary matrices are not arranged from fastest-varying to slowest.
To account for such discrepancies, hdf_utils has a very handy function that goes through each of the columns or
rows in the ancillary indices matrices and finds the order in which these dimensions vary.

Below we illustrate an example of sorting the names of the spectroscopic dimensions from fastest to slowest in
a BEPS data file:



.. code-block:: python


    spec_sort_order = px.hdf_utils.get_sort_order(h5_spec_inds)
    print('Spectroscopic dimensions arranged as is:')
    unsorted_spec_labels = px.hdf_utils.get_formatted_labels(h5_spec_inds)
    print(unsorted_spec_labels)
    sorted_spec_labels = np.array(unsorted_spec_labels)[np.array(spec_sort_order)]
    print('Spectroscopic dimensions arranged from fastest to slowest')
    print(sorted_spec_labels)





.. rst-class:: sphx-glr-script-out

 Out::

    Spectroscopic dimensions arranged as is:
    ['Frequency (Hz)', 'DC_Offset (V)', 'Field ()', 'Cycle ()']
    Spectroscopic dimensions arranged from fastest to slowest
    ['Frequency (Hz)' 'Field ()' 'DC_Offset (V)' 'Cycle ()']


When visualizing the data it is essential to plot the data against appropriate values on the X, Y, Z axes.
Extracting a simple list or array of values to plot against may be challenging especially for multidimensional
dataset such as the one under consideration. Fortunately, hdf_utils has a very handy function for this as well:



.. code-block:: python


    h5_spec_inds = px.hdf_utils.getAuxData(pycro_main, auxDataName='Spectroscopic_Indices')[0]
    h5_spec_vals = px.hdf_utils.getAuxData(pycro_main, auxDataName='Spectroscopic_Values')[0]
    dimension_name = 'DC_Offset'
    dc_dict = px.hdf_utils.get_unit_values(h5_spec_inds, h5_spec_vals, dim_names=dimension_name)
    print(dc_dict)
    dc_val = dc_dict[dimension_name]

    fig, axis = plt.subplots()
    axis.plot(dc_val)
    axis.set_title(dimension_name)
    axis.set_xlabel('Points in dimension')




.. image:: /auto_publications/images/sphx_glr_plot_tutorial_01_interacting_w_h5_files_001.png
    :align: center


.. rst-class:: sphx-glr-script-out

 Out::

    {'DC_Offset': array([ 0. ,  0.5,  1. ,  1.5,  2. ,  2.5,  3. ,  3.5,  4. ,  4.5,  5. ,
            5.5,  6. ,  6.5,  7. ,  7.5,  8. ,  7.5,  7. ,  6.5,  6. ,  5.5,
            5. ,  4.5,  4. ,  3.5,  3. ,  2.5,  2. ,  1.5,  1. ,  0.5,  0. ,
           -0.5, -1. , -1.5, -2. , -2.5, -3. , -3.5, -4. , -4.5, -5. , -5.5,
           -6. , -6.5, -7. , -7.5, -8. , -7.5, -7. , -6.5, -6. , -5.5, -5. ,
           -4.5, -4. , -3.5, -3. , -2.5, -2. , -1.5, -1. , -0.5], dtype=float32)}


Yet again, this process is simpler when using the PycroDataset object:



.. code-block:: python


    dv_val = pycro_main.get_spec_values(dim_name=dimension_name)

    fig, axis = plt.subplots()
    axis.plot(dc_val)
    axis.set_title(dimension_name)
    axis.set_xlabel('Points in dimension')




.. image:: /auto_publications/images/sphx_glr_plot_tutorial_01_interacting_w_h5_files_002.png
    :align: center




Reshaping Data
==============

Pycroscopy stores N dimensional datasets in a flattened 2D form of position x spectral values. It can become
challenging to retrieve the data in its original N-dimensional form, especially for multidimensional datasets
such as the one we are working on. Fortunately, all the information regarding the dimensionality of the dataset
are contained in the spectral and position ancillary datasets. hdf_utils has a very useful function that can
help retrieve the N-dimensional form of the data using a simple function call:



.. code-block:: python


    ndim_form, success, labels = px.hdf_utils.reshape_to_Ndims(h5_raw, get_labels=True)
    if success:
        print('Succeeded in reshaping flattened 2D dataset to N dimensions')
        print('Shape of the data in its original 2D form')
        print(h5_raw.shape)
        print('Shape of the N dimensional form of the dataset:')
        print(ndim_form.shape)
        print('And these are the dimensions')
        print(labels)
    else:
        print('Failed in reshaping the dataset')





.. rst-class:: sphx-glr-script-out

 Out::

    Succeeded in reshaping flattened 2D dataset to N dimensions
    Shape of the data in its original 2D form
    (25, 22272)
    Shape of the N dimensional form of the dataset:
    (5, 5, 87, 64, 2, 2)
    And these are the dimensions
    ['X' 'Y' 'Frequency' 'DC_Offset' 'Field' 'Cycle']


The whole process is simplified further when using the PycroDataset object:



.. code-block:: python


    ndim_form = pycro_main.get_n_dim_form()
    print('Shape of the N dimensional form of the dataset:')
    print(ndim_form.shape)
    print('And these are the dimensions')
    print(pycro_main.n_dim_labels)





.. rst-class:: sphx-glr-script-out

 Out::

    Shape of the N dimensional form of the dataset:
    (5, 5, 87, 64, 2, 2)
    And these are the dimensions
    ['X', 'Y', 'Frequency', 'DC_Offset', 'Field', 'Cycle']



.. code-block:: python

    two_dim_form, success = px.hdf_utils.reshape_from_Ndims(ndim_form,
                                                            h5_pos=h5_pos_inds,
                                                            h5_spec=h5_spec_inds)
    if success:
        print('Shape of flattened two dimensional form')
        print(two_dim_form.shape)
    else:
        print('Failed in flattening the N dimensional dataset')





.. rst-class:: sphx-glr-script-out

 Out::

    Shape of flattened two dimensional form
    (25, 22272)



.. code-block:: python


    # Close and delete the h5_file
    h5_f.close()
    os.remove(h5_path)






**Total running time of the script:** ( 0 minutes  0.723 seconds)



.. only :: html

 .. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: plot_tutorial_01_interacting_w_h5_files.py <plot_tutorial_01_interacting_w_h5_files.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: plot_tutorial_01_interacting_w_h5_files.ipynb <plot_tutorial_01_interacting_w_h5_files.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
